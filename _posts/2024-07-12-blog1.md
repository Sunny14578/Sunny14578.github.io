---
title: "Local에서 LLM 모델 사용해보기 ( 무료!! ) - 1"
date: "2024-07-12 00:40:00 +0900"
categories: [LangChain]
tags: [LLM, LangChain]
url: "posts/blog1"
---
출처 : 테디노트

<!-- check -->
## LangServe를 이용해 로컬에서 LLM 모델 사용해보기

### 설치
<hr>

#### 올라마(Ollama)
[올라마 Link](https://ollama.com/) 에서 중앙에 있는 다운로드 버튼으로 별도의 설정없이 진행

![](https://github.com/user-attachments/assets/47a763fb-91d4-4a6e-9342-eb187866c146)


#### HugginFace Hub 설치

```python
pip install hugginface-hub
```

#### EEVE(야놀자) model 다운로드

[HugginFace Page](https://huggingface.co/teddylee777/EEVE-Korean-Instruct-10.8B-v1.0-gguf/tree/main) 페이지에서 `EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf`모델을 다운로드 받아주자
<br> Q{num} num에 따라 모델의 크기가 다른데 8이 가장 큰 버전으로 성능이 가장 좋다. <br> 본인 pc가 감당할 수 있는 크기의 모델을 선택하면 된다.

![](https://github.com/user-attachments/assets/9783d05f-7de0-4cb5-ad6e-100027856cbc)

### Modelfile
[Model File 문서](https://github.com/ollama/ollama/blob/69f392c9b7ea7c5cc3d46c29774e37fdef51abd8/docs/modelfile.md)

model을 Ollama에서 사용하게 하기 위해 ModelFile을 구성한다. <br> Modelfile 이름의 파일을 생성한 후 아래의 코드 복사 붙여넣기

FROM 이후의 gguf 파일은 우리가 다운로드한 모델의 이름으로 적용한다. <br> 허깅페이스 페이지에서 `EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf`를 다운로드 받았기 때문에 해당 명으로 적용

```python
FROM EEVE-Korean-Instruct-10.8B-v1.0-Q8_0.gguf

TEMPLATE """{{- if .System }}
<s>{% raw %}{{ .System }}{% endraw %}</s>
{{- end }}
<s>Human:
{% raw %}{{ .Prompt }}{% endraw %}</s>
<s>Assistant:
"""

SYSTEM """A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."""

PARAMETER stop <s>
PARAMETER stop </s>
```

필자는 아래와 같이 EEVE model file과 Modelfile을 세팅했다.

![](https://github.com/user-attachments/assets/5ddd63e2-393b-4f1d-b07d-e4015231bb2f)

Modelfile에 따라서 답변의 퀄리티가 달라질 수 있다. Modelfile 까지 준비가 됐다면 
ollama에 세팅을 하기위해 밑의 코드를 터미널에서 실행한다.

```shell
ollama create EEVE -f ./Modelfile
```


명령어 규칙은 ollama create [내가사용할이름] -f [Modelfile 경로]이다. <br> 이름은 EEVE로 사용했고 Modelfile 경로는 현재폴더 위치의 Modelfile로 설정했다.
 <br> 잘 생성되었는지 밑의 코드를 통해 확인해보자. 

```shell
ollama list
```

![](https://github.com/user-attachments/assets/e1f8a421-97a9-492e-833c-aed11f2442e1)

### Model Test

밑 코드를 통해 터미널상에서 모델을 실행하고 질문을 하면 답변을 얻을 수 있다.
```shell
ollama run EEVE:latest
```

![](https://github.com/user-attachments/assets/3411222e-ca9e-472f-a68b-8729b76008c3)


여기까지 허깅페이스페이지에서 모델을 다운받아 ollama에 세팅하여 터미널상에서 모델을 실행하여 질문과 답변을 할 수 있었다.
다음 포스팅에서는 web상에서 해당 기능을 이용해보는 과정을 알아보도록 하자!

